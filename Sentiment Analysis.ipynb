{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE 5522 Lab2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4bzjvrI1J8b",
        "colab_type": "text"
      },
      "source": [
        "**Name: Xuecheng Liu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPwb8ny-hOU",
        "colab_type": "text"
      },
      "source": [
        "**CSE 5522 Hands-on #2: Naive Bayes**\n",
        "\n",
        "The goals of today's exercise are to familarize you with:\n",
        "\n",
        "\n",
        "*   Naive Bayes\n",
        "*   Binary Classification\n",
        "*   Data exploration\n",
        "\n",
        "**END OF CLASS GOAL:** Submit a link to your notebook (Share > Get Sharable Link) in Carmen so I can see how far you got.  This should be submitted in a group assignment page on Carmen.\n",
        "\n",
        "**Part 0: Initial setup**\n",
        "\n",
        "**0.0:** If none of your team members are familar with python this will be difficult to accomplish - you may want to split up and join different groups.\n",
        "\n",
        "**0.1:** Go to the Carmen Page for CSE 5522, and find the Group signup tab.  Choose a group under HandsOn2-xx (where xx=1 to 20), and sign your group members up.  This will allow you to submit a group assignment at the end.\n",
        "\n",
        "**0.2:** Make a copy of this page in your google drive so that you can edit it. Edit the filename to include your group number.  Share the copied page with your teammates. At the end of class, share a URL and submit (so I can see how far you got).  This will count as the participation grade for all members.\n",
        "\n",
        "**0.3:** While not completely necessary for this assignment, you may want to familiarize yourself with the following packages: [numpy](https://numpy.org), [scikit-learn](https://scikit-learn.org), [pandas](https://pandas.pydata.org), [matplotlib](https://matplotlib.org).\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct",
        "colab_type": "text"
      },
      "source": [
        "**Part 1: A Simple Bayes Net: Naive Bayes**\n",
        "\n",
        "In class, we discussed how conditional independences of a joint probablity distribution get encoded by a Bayesian Network. One of the simplest form of BNs is the Naive Bayes model which encodes a set of simple conditional independences: \n",
        "\n",
        "- Given a single cause all of the effects are independent from each other.\n",
        "- Mathematically: \n",
        "$P($*cause*$, $*effect*$_1, ..., $*effect*$_n) = P($*cause*$) \\prod_i P($*effect*$_i|$*cause*$)$ \n",
        "\n",
        "NB can be used for classification by assuming that cause is the true (unknown) label and it (probabilistically) generates all of the features (effects) while features are independent given the cause. \n",
        "\n",
        "For example, in sentiment analysis the *cause* is the author's sentiment (say, unknown label from the set of {sad, happy, feared, suprised, disgusted, angry}) and the *effects* are words that s/he writes. The simplifying assumption of NB says that knowing the latent sentiment, words of the sentence are independent. We know this assumption is not true because grammar and word-use impose some dependency structure between words in the sentence, but we choose to ignore that in this model.\n",
        "\n",
        "Although simple, NB has shown good performance in many classifcation tasks and has become a standard classic baseline for classification. \n",
        "\n",
        "Today we want to perform Twitter sentiment analysis using NB. The goal is to figure out if a tweet has a positive or negative sentiment about the weather.  \n",
        "\n",
        "**1.0:** Set up the environment (you can click on the play button below to import the appropriate modules)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fYsm5f-UbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-MaOhV5UUvD",
        "colab_type": "text"
      },
      "source": [
        "**1.1** Read the data from GitHub into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HskImt85UhU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvgt0PYLVHYr",
        "colab_type": "text"
      },
      "source": [
        "**1.2** Print out the top of the dataframe to make sure that the data loaded correctly.  It should be a data table with three columns (weight, tweet, label), and 3697 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScERznWSVBK3",
        "colab_type": "code",
        "outputId": "ea5efcaf-f569-4ab8-eadf-292a0f060cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 366
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AssaBgdR0K",
        "colab_type": "text"
      },
      "source": [
        "**1.3.** In the next step, we should build our feature matrix by converting the string of words to a vector of numeric values. \n",
        "\n",
        "First we need to assign a unique id to each word and create the feature matrix with correct size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q7tGhlVcOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkM_oBZ7cv7",
        "colab_type": "text"
      },
      "source": [
        "Checking head of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdK4g_D8hX-4",
        "outputId": "0b0374a5-e32a-476d-acb6-c35cf4856900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "dict(list(wordDict.items())[0:10])"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 9,\n",
              " 'be': 7,\n",
              " 'cold': 3,\n",
              " 'is': 1,\n",
              " 'it': 0,\n",
              " 'out': 4,\n",
              " 'to': 6,\n",
              " 'very': 2,\n",
              " 'want': 5,\n",
              " 'warmer': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmXPr6qeSQo",
        "colab_type": "text"
      },
      "source": [
        "**1.4:** The simplest way of coding a tweet to numbers is to mark the occurrence of a word, and forget about its frequency in the document (tweet). This works well with tweets as there are not many repetitive words in a single tweet. So let's fill the document-word matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap5o8fzI7rgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714iX2JA9XMh",
        "colab_type": "text"
      },
      "source": [
        "Now we check if the number of words are correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiap5wBW86lZ",
        "colab_type": "code",
        "outputId": "9c44fb0a-9fe3-4c65-b1ab-2bb372b16360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.sum(X[0:5, ], axis = 1)"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  9., 17.,  9.,  4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 370
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPNK1Su-Hwf",
        "colab_type": "text"
      },
      "source": [
        "Finally, we extract the labels from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGm_x8Nm-HL6",
        "colab_type": "code",
        "outputId": "91fab9c7-0f5a-4792-e1ad-cf88e4fd494f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ-EgzBo-wLd",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the total number of positive and negative tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFKKNsM7-_UN",
        "colab_type": "code",
        "outputId": "164d6ae2-5a5f-4647-fdd9-9ff297d7c9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "numNeg = np.sum(y<0)\n",
        "numPos = np.sum(y>=0) #len(y) - numNeg\n",
        "probNeg = numNeg / (numNeg + numPos)\n",
        "probPos = 1 - probNeg\n",
        "display(numNeg, numPos, probNeg, probPos)"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1650"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2047"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4463078171490398"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5536921828509602"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlOIWDiI7JQw",
        "colab_type": "text"
      },
      "source": [
        "So samples 0:1649 are negative and 1650:-1 are positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s3N0xJOILK",
        "colab_type": "text"
      },
      "source": [
        "**1.5: Train/Test Split** Now with do the 20/80 split and learn the word probabilities using the 80 % part and test the NB performance on the 20 % part. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdLXcY0PCQX",
        "colab_type": "code",
        "outputId": "6c2c4297-7337-46e6-c2d5-ba9c7e618ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)\n",
        "#Note: random_state=0 fixes the random seed so we get the same split every run. Don't use this below"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2LTz2CP95GT",
        "colab_type": "text"
      },
      "source": [
        "**1.6: Computing Probabilities by Counting** Now the real work begins. Write the code that, from the train feature matrix xTrain computes the needed word probabilites, i.e., $P(word|label)$ where label is + or - and word is any of the words saved in the `wordDict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ7KjLPk7oMZ",
        "colab_type": "code",
        "outputId": "c0b7d370-8fc2-4c10-ac7e-c80925a199b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# compute three distributions (four variables):\n",
        "def compute_distros(x,y):\n",
        "  # probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "  probWordGivenPositive=np.sum(x[y>=0,:],axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
        "  probWordGivenPositive=probWordGivenPositive/np.sum(y>=0) #Divide by total number of (positive) examples to give distribution\n",
        "\n",
        "  # probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "  probWordGivenNegative=np.sum(x[y<0,:],axis=0)\n",
        "  probWordGivenNegative=probWordGivenNegative/np.sum(y<0)\n",
        "\n",
        "  # priorPositive: P(Sentiment = +ive)\n",
        "  priorPositive = np.sum(y>=0)/y.shape[0] #Number of positive examples vs. all examples\n",
        "  # priorNegative: P(Sentiment = -ive)\n",
        "  priorNegative = 1 - priorPositive\n",
        "  #  (note these last two form one distribution)\n",
        "\n",
        "  return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "# compute distributions here\n",
        "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "\n",
        "# checking the results\n",
        "display(probWordGivenPositive[0:5])\n",
        "display(probWordGivenNegative[0:5])\n",
        "display(priorPositive, priorNegative)"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.1185006 , 0.20737606, 0.01088271, 0.01451028, 0.10217654])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.14504988, 0.19493477, 0.00537222, 0.09669992, 0.13967767])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.5593506932702063"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.44064930672979374"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUFU8eUQ_8gC",
        "colab_type": "text"
      },
      "source": [
        "Note that you only needed to compute $P(word = 1| +)$ or $P(word = 1| -)$ and the probabilities of the word being absent from a tweet is just 1 minus those probabilities. \n",
        "\n",
        "However, as we see in 1.7, for convenience, we will also want to compute $log P(word = 1 | +)$, $log P(word = 0 | +)$, $log P(word = 1 | -)$ and $log P(word = 0 | -)$.  Also we should compute the log priors.  Let's do so now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLcaaDTiwF0",
        "colab_type": "code",
        "outputId": "86daee3d-0e22-4958-cef6-25f49da068d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# compute the following:\n",
        "# logProbWordPresentGivenPositive\n",
        "# logProbWordAbsentGivenPositive\n",
        "# logProbWordPresentGivenNegative\n",
        "# logProbWordAbsentGivenNegative\n",
        "# logPriorPositive\n",
        "# logPriorNegative\n",
        "def compute_logdistros(distros, min_prob):\n",
        "  if True:\n",
        "    #Assume missing words are simply very rare\n",
        "    #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
        "    distros=np.where(distros>=min_prob,distros,min_prob)\n",
        "    #Also need to consider minimum probability for \"not\" distribution\n",
        "    distros=np.where(distros<=(1-min_prob),distros,1-min_prob)\n",
        "\n",
        "    return np.log(distros), np.log(1-distros)\n",
        "  else:\n",
        "    #Ignore missing words (assume they have P==1, i.e. force log 0 to 0)\n",
        "    return np.log(np.where(distros>0,distros,1)), np.log(np.where(distros<1,1-distros,1))\n",
        "\n",
        "min_prob = 1/yTrain.shape[0] #Assume very rare words only appeared once\n",
        "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "\n",
        "# Did this work, or did you get an error?  (Read below.)\n",
        "display(logProbWordPresentGivenPositive[0:5])\n",
        "display(logProbWordAbsentGivenPositive[0:5])\n",
        "display(logProbWordPresentGivenNegative[0:5])\n",
        "display(logProbWordAbsentGivenNegative[0:5])\n",
        "display(logPriorPositive, logPriorNegative)"
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-2.13283722, -1.57322143, -4.52058012, -4.23289805, -2.28105316])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.12613096, -0.23240639, -0.01094236, -0.01461658, -0.10778182])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1.93067756, -1.63509031, -5.22651443, -2.33614267, -1.96841789])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.15671216, -0.21683197, -0.0053867 , -0.10170047, -0.15044815])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.5809786442688406"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-0.819505942727632"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXVQ7ZHAkH1u",
        "colab_type": "text"
      },
      "source": [
        "You likely received an error when you tried to take $log(0)$ at some point.  Can your group think of a way to avoid taking $log(0)$?  Check in with your instructor/TA to see if what you're thinking will work.  Implement that change in your code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJgUmRcDNyCV",
        "colab_type": "text"
      },
      "source": [
        "**Lab 2 Part 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKPHzlFgHuQ",
        "colab_type": "text"
      },
      "source": [
        "We first do the prediction without incorperating absent words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vwCEjBUN_Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# predict the label using Naive Bayes\n",
        "def predict(xTest,logProbWordPresentGivenPositive,logProbWordPresentGivenNegative,logPriorPositive, logPriorNegative):\n",
        "  pred = []\n",
        "  for i in range(xTest.shape[0]):\n",
        "    probPositive = xTest[i,:] * logProbWordPresentGivenPositive\n",
        "    probPositive = np.sum(probPositive)\n",
        "    probPositive = probPositive + logPriorPositive\n",
        "    \n",
        "    probNegetive = xTest[i,:] * logProbWordPresentGivenNegative\n",
        "    probNegetive = np.sum(probNegetive)\n",
        "    probNegetive = probNegetive + logPriorNegative\n",
        "    if probPositive - probNegetive >= 0:\n",
        "      pred.append(1)\n",
        "    else:\n",
        "      pred.append(-1)\n",
        "  return pred\n",
        "\n",
        "pred = predict(xTest,logProbWordPresentGivenPositive,logProbWordPresentGivenNegative,logPriorPositive, logPriorNegative)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOsbvBwzijcA",
        "colab_type": "text"
      },
      "source": [
        "Now we take absent word into consideration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Gp47J0ijnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def predict_Absent(xTest,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,logPriorPositive, logPriorNegative):\n",
        "  pred = []\n",
        "  for i in range(xTest.shape[0]):\n",
        "    probPositive = xTest[i,:] * logProbWordPresentGivenPositive\n",
        "    probPositive = np.sum(probPositive)\n",
        "    probPositive = probPositive + logPriorPositive\n",
        "    probPositive += np.sum((1-xTest[i,:]) * logProbWordAbsentGivenPositive)\n",
        "    \n",
        "    probNegetive = xTest[i,:] * logProbWordPresentGivenNegative \n",
        "    probNegetive = np.sum(probNegetive)\n",
        "    probNegetive = probNegetive + logPriorNegative\n",
        "    probNegetive += np.sum((1-xTest[i,:]) * logProbWordAbsentGivenNegative)\n",
        "    \n",
        "    if probPositive >= probNegetive:\n",
        "      pred.append(1)\n",
        "    else:\n",
        "      pred.append(-1)\n",
        "  return pred\n",
        "\n",
        "pred_absent = predict_Absent(xTest,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,logPriorPositive, logPriorNegative)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k4Su31EtWJU",
        "colab_type": "text"
      },
      "source": [
        "**Calculate the accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNIgiD16swH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(pred,yTest):\n",
        "  correct = 0\n",
        "  for i in range(yTest.shape[0]):\n",
        "    if pred[i] == yTest[i]:\n",
        "      correct+=1\n",
        "  return correct/yTest.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsV9TDIuirq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4622ece0-eca6-4195-d687-ab747b3aba21"
      },
      "source": [
        "acc = accuracy(pred,yTest)\n",
        "acc_absent = accuracy(pred_absent,yTest)\n",
        "print(\"The accuracy for not using absent word to predict is: \", acc)\n",
        "print(\"The accuracy for using absent word to predict is: \", acc_absent)"
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for not using absent word to predict is:  0.8283783783783784\n",
            "The accuracy for using absent word to predict is:  0.8297297297297297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81PPMK-tp-b",
        "colab_type": "text"
      },
      "source": [
        "**As you can see from the result above, the accuracy with using absent word is slightly higher than without considering absent word**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j_itRHZt7cv",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "The labels each came with a weight.  Devise a method for weighting samples, and use that method to recalculate the probability distributions.  Report the effect of weighting samples on the test set. (40 points + 2 bonus points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQDD95lvuTfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_weighted(pred,yTest):\n",
        "  correct = 0\n",
        "  for i in range(yTest.shape[0]):\n",
        "    if pred[i] * yTest[i] > 0 :\n",
        "      correct+=1\n",
        "  return correct/yTest.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eemO31F2FTU",
        "colab_type": "text"
      },
      "source": [
        "**incoperating weight into data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI1ArtoI2Dw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = np.array(tweet_dataframe.iloc[:,0])\n",
        "\n",
        "X = X * weight.reshape(-1,1)\n",
        "y = y * weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ5lWSV_uOfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7645ef05-c64d-4bf4-b451-7adf9e5a617c"
      },
      "source": [
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, shuffle= True)\n",
        "min_prob = 1/np.sum(np.abs(y)) \n",
        "probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = compute_distros(xTrain,yTrain)\n",
        "logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive = compute_logdistros(probWordGivenPositive,min_prob)\n",
        "logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative = compute_logdistros(probWordGivenNegative,min_prob)\n",
        "logPriorPositive, logPriorNegative = compute_logdistros(priorPositive,min_prob)\n",
        "pred = predict(xTest,logProbWordPresentGivenPositive,logProbWordPresentGivenNegative,logPriorPositive, logPriorNegative)\n",
        "pred_absent = predict_Absent(xTest,logProbWordPresentGivenPositive, logProbWordAbsentGivenPositive,logProbWordPresentGivenNegative, logProbWordAbsentGivenNegative,logPriorPositive, logPriorNegative)\n",
        "  \n",
        "c = accuracy_weight(pred,yTest)\n",
        "c_absent = accuracy_weight(pred_absent,yTest)\n",
        "print(\"The accuracy of prediction without absent word(weighted) is: \",c)\n",
        "print(\"The accuracy of prediction using absent word(weighted) is: \",c_absent)\n",
        " "
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean of the accuracy of prediction without absent word(weighted) is:  0.8094594594594594\n",
            "The mean of the accuracy of prediction using absent word(weighted) is:  0.822972972972973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiyQJZOE2X5m",
        "colab_type": "text"
      },
      "source": [
        "**Afer adding the weight as a new feature, the accuracy of the model decreased for both conditions**"
      ]
    }
  ]
}