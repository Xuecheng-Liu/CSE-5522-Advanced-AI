{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE5522_lab4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFIsTXUV21Io",
        "colab_type": "text"
      },
      "source": [
        "**CSE 5522 Lab 4**\n",
        "**Xuecheng Liu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS2qozEIIOh9",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**: Using the vowel data from Part 2 of the HandsOn, construct a 2-layer MLP with 100 hidden units, predicting the vowel class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvJzrTbOZlZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "url='https://raw.githubusercontent.com/efosler/cse5522data/master/vowelfmts.csv'\n",
        "df=pd.read_csv(url)\n",
        "# prepare the data\n",
        "vowels = df['vowel'].unique()\n",
        "vmap = {} # key 0.0, value = 'ah'\n",
        "for i in range(vowels.shape[0]):\n",
        "  vmap[vowels[i]]=i\n",
        "df['label']=df['vowel'].map(vmap)\n",
        "df = df.drop(columns = ['vowel'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l8J-1KnkynB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get data in numpy array format\n",
        "X = np.array(df.iloc[:,0:2],dtype='float')\n",
        "Y = np.array(df.iloc[:,10],dtype='float')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeToNErZqije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize the data\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.normalize(X, norm='l2')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKxWSsOLZOXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "a, b, c, d = train_test_split(X,Y,test_size=0.2,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RobQ3gdlWP-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUpM0YOUq1uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert numpy array to tensor\n",
        "X_train = torch.from_numpy(a).type(torch.FloatTensor)\n",
        "X_test = torch.from_numpy(b).type(torch.FloatTensor)\n",
        "Y_train = torch.from_numpy(c).type(torch.LongTensor)\n",
        "Y_test = torch.from_numpy(d).type(torch.LongTensor)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nAqjh3psQUw",
        "colab_type": "text"
      },
      "source": [
        "data processing done here. Now starts to build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1MMmt7JHqFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP2(nn.Module):\n",
        "    def __init__(self,input_dim,hid_dim ,output_dim):\n",
        "        # initialze the superclass\n",
        "        super(MLP2, self).__init__()\n",
        "        self.lin1 = nn.Linear(input_dim,hid_dim) \n",
        "        self.lin2 = nn.Linear(hid_dim,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.lin1(x)\n",
        "      x = torch.sigmoid(x)\n",
        "      x = self.lin2(x)\n",
        "      #x = torch.softmax(x)\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfCnXzPAtY-b",
        "colab_type": "text"
      },
      "source": [
        "define a function to initialize weights of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUOZi93zYpnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # initialize the weight tensor, here we use a normal distribution\n",
        "            m.weight.data.normal_(0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-a44U26tSw2",
        "colab_type": "text"
      },
      "source": [
        "Now create a model and initialize it. Also define a loss function and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpxxmeFYxrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP2(2,100,9)\n",
        "weights_init(model)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-jcQoWotdAx",
        "colab_type": "text"
      },
      "source": [
        "now start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdZo-gsDZDfC",
        "colab_type": "code",
        "outputId": "2b2743e9-dcaa-41fa-f96a-57c96c9383f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "epochs = 2001\n",
        "steps = X_train.size(0)\n",
        "for i in range(epochs):\n",
        "  #for j in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model(X_train)\n",
        "    loss = loss_func.forward(y_hat, Y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))\n"
      ],
      "execution_count": 533,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 4.636462688446045, \n",
            "Epoch: 500, Loss: 1.6070362329483032, \n",
            "Epoch: 1000, Loss: 1.5837559700012207, \n",
            "Epoch: 1500, Loss: 1.5785245895385742, \n",
            "Epoch: 2000, Loss: 1.5766174793243408, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GTTZ3CBf7fZ",
        "colab_type": "code",
        "outputId": "abb8e403-4215-4f27-9d3f-5b05d39d9231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predict = model(X_test)\n",
        "predict = torch.argmax(predict,dim=1)\n",
        "acc = 0\n",
        "for i in range(predict.size(0)):\n",
        "  if predict[i] == Y_test[i]:\n",
        "    acc = acc + 1\n",
        "acc = (acc/predict.size(0))*100\n",
        "print(\"The accuracy on the test data is {0}%.\".format(acc))"
      ],
      "execution_count": 534,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy on the test data is 37.65432098765432%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d-3IfaUsw26",
        "colab_type": "text"
      },
      "source": [
        "**As you have seen above, the model did a relatively poor job on the testing data. I will perform two experiments to the model to see how those affect the performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HcLUGk8tAIS",
        "colab_type": "text"
      },
      "source": [
        "**For the first experiment, I will add one more hidden layer with 100 hidden units**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4v3QwBxs-N2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP3(nn.Module):\n",
        "    def __init__(self,input_dim,hid_dim1,hid_dim2,output_dim):\n",
        "        # initialze the superclass\n",
        "        super(MLP3, self).__init__()\n",
        "        self.lin1 = nn.Linear(input_dim,hid_dim1) \n",
        "        self.lin2 = nn.Linear(hid_dim1,hid_dim2)\n",
        "        self.lin3 = nn.Linear(hid_dim2,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.lin1(x)\n",
        "      x = torch.sigmoid(x)\n",
        "      x = self.lin2(x)\n",
        "      x = torch.sigmoid(x)\n",
        "      x = self.lin3(x)\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC4rM1A4t8bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = MLP3(2,100,100,9)\n",
        "weights_init(model2)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.05, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3kxRquGucE2",
        "colab_type": "code",
        "outputId": "4b52aa6f-faf7-4064-e197-8d861f8cbee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "epochs = 2001\n",
        "steps = X_train.size(0)\n",
        "for i in range(epochs):\n",
        "  #for j in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model2(X_train)\n",
        "    loss = loss_func.forward(y_hat, Y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": 537,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 15.887540817260742, \n",
            "Epoch: 500, Loss: 1.569237232208252, \n",
            "Epoch: 1000, Loss: 1.5507652759552002, \n",
            "Epoch: 1500, Loss: 1.5337105989456177, \n",
            "Epoch: 2000, Loss: 1.515975832939148, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvDbWfecuquZ",
        "colab_type": "code",
        "outputId": "0dec9a3f-37a4-4964-837f-a0003f4941be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predict = model2(X_test)\n",
        "predict = torch.argmax(predict,dim=1)\n",
        "acc = 0\n",
        "for i in range(predict.size(0)):\n",
        "  if predict[i] == Y_test[i]:\n",
        "    acc = acc + 1\n",
        "acc = (acc/predict.size(0))*100\n",
        "print(\"The accuracy on the test data is {0}%.\".format(acc))"
      ],
      "execution_count": 538,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy on the test data is 39.04320987654321%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUGX4dGivV4z",
        "colab_type": "text"
      },
      "source": [
        "**After adding one more hidden layer with 100 units, the performance gets improved a little bit. However, it still does a relatively poor job. The reason why I choose to add one more hidden layer dues to the nature of deep learning: as the depth increased, the model can better learn of data so that to make better prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xsB5vBhwF3k",
        "colab_type": "text"
      },
      "source": [
        "**For the second experiment, I will modify the activation function from the original model. The reason why I choose to tune the activation functions is that different activation functions may somehow affects the input to the hidden layer and result in different performance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBwUa0SX2gFg",
        "colab_type": "text"
      },
      "source": [
        "**I will change sigmoid to relu in the first model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cEMYnj2voNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP4(nn.Module):\n",
        "    def __init__(self,input_dim,hid_dim ,output_dim):\n",
        "        # initialze the superclass\n",
        "        super(MLP4, self).__init__()\n",
        "        self.lin1 = nn.Linear(input_dim,hid_dim) \n",
        "        self.lin2 = nn.Linear(hid_dim,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.lin1(x)\n",
        "      x = torch.relu(x) # changing here from sigmoid to relu\n",
        "      x = self.lin2(x)\n",
        "      return x\n",
        "\n",
        "model3 = MLP4(2,100,9)\n",
        "weights_init(model3)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model3.parameters(), lr=0.05, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWv52F65wk3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "03de7439-91b3-44f8-8e7a-4cd3de26026e"
      },
      "source": [
        "epochs = 2001\n",
        "steps = X_train.size(0)\n",
        "for i in range(epochs):\n",
        "  #for j in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model3(X_train)\n",
        "    loss = loss_func.forward(y_hat, Y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": 540,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 9.176797866821289, \n",
            "Epoch: 500, Loss: 1.5447214841842651, \n",
            "Epoch: 1000, Loss: 1.5267431735992432, \n",
            "Epoch: 1500, Loss: 1.508254051208496, \n",
            "Epoch: 2000, Loss: 1.493236780166626, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsvtySjWxEN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c84fdc54-f45d-45d9-85fe-066685c6600a"
      },
      "source": [
        "predict = model3(X_test)\n",
        "predict = torch.argmax(predict,dim=1)\n",
        "acc = 0\n",
        "for i in range(predict.size(0)):\n",
        "  if predict[i] == Y_test[i]:\n",
        "    acc = acc + 1\n",
        "acc = (acc/predict.size(0))*100\n",
        "print(\"The accuracy on the test data is {0}%.\".format(acc))"
      ],
      "execution_count": 541,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy on the test data is 39.04320987654321%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQFD9mNzDQY",
        "colab_type": "text"
      },
      "source": [
        "**now testing with tanh activation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7LQ2ylzC0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP5(nn.Module):\n",
        "    def __init__(self,input_dim,hid_dim ,output_dim):\n",
        "        # initialze the superclass\n",
        "        super(MLP5, self).__init__()\n",
        "        self.lin1 = nn.Linear(input_dim,hid_dim) \n",
        "        self.lin2 = nn.Linear(hid_dim,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.lin1(x)\n",
        "      x = torch.tanh(x) # changed to tanh\n",
        "      x = self.lin2(x)\n",
        "      return x\n",
        "\n",
        "model4 = MLP5(2,100,9)\n",
        "weights_init(model4)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model4.parameters(), lr=0.05, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zakbJKZRzVuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "3c1be93f-5c59-4f10-87a7-26937059c36c"
      },
      "source": [
        "epochs = 2001\n",
        "steps = X_train.size(0)\n",
        "for i in range(epochs):\n",
        "  #for j in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model4(X_train)\n",
        "    loss = loss_func.forward(y_hat, Y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        print(\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": 543,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 6.163071155548096, \n",
            "Epoch: 500, Loss: 1.5693860054016113, \n",
            "Epoch: 1000, Loss: 1.5618906021118164, \n",
            "Epoch: 1500, Loss: 1.5547130107879639, \n",
            "Epoch: 2000, Loss: 1.5475351810455322, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4D6VZqqzf-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82a39171-8589-4762-ed6e-e2e137c8e717"
      },
      "source": [
        "predict = model4(X_test)\n",
        "predict = torch.argmax(predict,dim=1)\n",
        "acc = 0\n",
        "for i in range(predict.size(0)):\n",
        "  if predict[i] == Y_test[i]:\n",
        "    acc = acc + 1\n",
        "acc = (acc/predict.size(0))*100\n",
        "print(\"The accuracy on the test data is {0}%.\".format(acc))"
      ],
      "execution_count": 544,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy on the test data is 37.65432098765432%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mag1hvf_zj8C",
        "colab_type": "text"
      },
      "source": [
        "**After trying sigmoid,relu and tanh respectively as activation function. I observer they end up with similar during the training (around 1.55).The accuracy over the testing set is similar with each other (around 37.5%).This experiments shows changing the activation function does not guarentee to have a better performance compared with adding more hidden layers**"
      ]
    }
  ]
}